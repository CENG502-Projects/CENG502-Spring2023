{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np              # NumPy, for working with arrays/tensors \n",
    "import time                     # For measuring time\n",
    "import random                   # Python's random library\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# PyTorch libraries:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Cuda (GPU support) is available and enabled!\")\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  print(\"Cuda (GPU support) is not available :(\")\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "###############################################################################################################\n",
    "## Writing Functions\n",
    "\n",
    "def windowed_Set(original_data, window_size, shifting,horizon):\n",
    "    time_sequence_length, n_dimensions  = original_data.shape\n",
    "    T = int((time_sequence_length - window_size-horizon) / shifting + 1)\n",
    "\n",
    "    windowedArray = np.empty((n_dimensions, T, window_size))\n",
    "    futuredArray  = np.empty((n_dimensions, T, horizon))\n",
    "\n",
    "    for d in range(n_dimensions):\n",
    "        for i in range(T):\n",
    "            start_index = i * shifting\n",
    "            window       = original_data[start_index : start_index + window_size,d]\n",
    "            horizon_data = original_data[start_index + window_size : start_index + window_size + horizon,d]\n",
    "            windowedArray[d, i, :] = window\n",
    "            futuredArray[d, i, :]  = horizon_data\n",
    "    return windowedArray, futuredArray\n",
    "\n",
    "def normalize_data(data_, desired_mean, desired_std):\n",
    "    \"\"\"This code turns the input into have the desired mean and variance values. When standard deviation is zero,\n",
    "       since division will be infinity, it will bypass.\n",
    "    \"\"\"\n",
    "    # Calculate the mean and variance of the original data\n",
    "    data = data_.copy()\n",
    "    original_mean = np.mean(data)\n",
    "    original_std = np.std(data)\n",
    "\n",
    "    # Subtract the mean from each data point\n",
    "    data -= original_mean\n",
    "    # Divide each data point by the square root of the variance\n",
    "    if original_std != 0:\n",
    "        data /= original_std\n",
    "        # Multiply each data point by the desired standard deviation\n",
    "        data *= desired_std\n",
    "    # Add the desired mean to each data point\n",
    "    data += desired_mean\n",
    "    return data\n",
    "  \n",
    "# CALCULATING MSE\n",
    "def calculate_mse(model, testloader):\n",
    "    model.eval()\n",
    "    mse = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            mse += torch.mean((outputs - labels) ** 2).item()\n",
    "\n",
    "    mse /= len(testloader)\n",
    "\n",
    "    return mse\n",
    "\n",
    "import torch\n",
    "\n",
    "def calculate_mae(model, testloader):\n",
    "    model.eval()\n",
    "    mae = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            mae += torch.mean(torch.abs(outputs - labels)).item()\n",
    "\n",
    "    mae /= len(testloader)\n",
    "\n",
    "    return mae\n",
    "\n",
    "def find_key(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        for i in range(len(val)):\n",
    "            if val[i] == value:\n",
    "                return key\n",
    "    return None  # Value not found in the dictionary\n",
    "\n",
    "class MarinaDataset_Etth(Dataset):\n",
    "    def __init__(self, vec,label):\n",
    "        self.vec = vec\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return self.vec.shape[1]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.vec[:,idx,:], self.label[:,idx,:]\n",
    "    \n",
    "class MarinaDataset_Smap(Dataset):\n",
    "    def __init__(self, vec,label):\n",
    "        self.vec = vec\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.vec)\n",
    "    def __getitem__(self, idx):        \n",
    "        return self.vec[idx], self.label[idx] \n",
    "\n",
    "###############################################################################################################\n",
    "# FORECASTING\n",
    "# For ETTH Dataset\n",
    "etth1_FILEPATH = \"./etth1/ETTh1.csv\"\n",
    "etth1 = pd.read_csv(etth1_FILEPATH);\n",
    "etth1.head(5)\n",
    "\n",
    "def trials_MARINA_Etth1(INPUT_LENGTH,zeta,shift):\n",
    "  ## Normalization Parameters\n",
    "  wL = INPUT_LENGTH     # window length\n",
    "  # Defining intermediate length and alpha before designing the modules\n",
    "\n",
    "  etthData = np.zeros((len(etth1[\"HUFL\"]),7))\n",
    "  for i, col in enumerate(etth1):\n",
    "      if col != 'date':\n",
    "          etthData[:,i-1] = etth1[col]\n",
    "  print(\"\\nEtth1 dataset has shape of:\", etthData.shape)\n",
    "\n",
    "  n_sequence, n_features  = etthData.shape\n",
    "  split_index = int(n_sequence * 0.8)  # 80% for training, 20% for testing\n",
    "\n",
    "  # Split the data into train and test sets\n",
    "  train_set = etthData[:split_index, :]\n",
    "  test_data = etthData[split_index:, :]\n",
    "\n",
    "  # Normalize train_set\n",
    "  for D_ in range(n_features):\n",
    "    train_set[:,D_]   = normalize_data(train_set[:,D_],0,1)\n",
    "\n",
    "  # Normalize test_set\n",
    "  for D_ in range(n_features):\n",
    "    test_data[:,D_]   = normalize_data(test_data[:,D_],0,1)\n",
    "\n",
    "  windowedTrainSet, futuredTrainSet = windowed_Set(train_set,wL,shift,zeta)\n",
    "  # print(\"Windowed array shape:\",windowedTrainSet.shape,\"[D,i,w]\",\n",
    "  #       \"\\nFutured array shape:\",futuredTrainSet.shape,\"[D,i,zeta]\")\n",
    "\n",
    "  windowedTest, futuredTest = windowed_Set(test_data,wL,shift,zeta)\n",
    "  # print(\"Windowed array shape:\",windowedTest.shape,\"[D,i,w]\",\n",
    "  #       \"\\nFutured array shape:\",futuredTest.shape,\"[D,i,zeta]\")\n",
    "\n",
    "  # Assuming your data is stored in the variable \"windowedTrainSet\"\n",
    "\n",
    "  # Generate random indices for the validation set\n",
    "  valP = 0.2  # Select 20% of the indices for validation\n",
    "  val_indices = random.sample(range(windowedTrainSet.shape[1]), int(valP * windowedTrainSet.shape[1]))\n",
    "\n",
    "  # Split the data into training and validation sets\n",
    "  windowedTrain = np.delete(windowedTrainSet, val_indices, axis=1)\n",
    "  futuredTrain  = np.delete(futuredTrainSet, val_indices, axis=1)\n",
    "  windowedVal   = windowedTrainSet[:, val_indices, :]\n",
    "  futuredVal    = futuredTrainSet[:, val_indices, :]\n",
    "\n",
    "  # Verify the shapes of the train and validation sets\n",
    "  print(\"Train data and futured data has shape of:\",windowedTrain.shape, futuredTrain.shape)\n",
    "  print(\"Validation data and futured data has shape of:\",windowedVal.shape, futuredVal.shape)\n",
    "  print(\"Test data and futured data has shape of:\",windowedTest.shape, futuredTest.shape)\n",
    "\n",
    "  return windowedTrain,futuredTrain,windowedVal,futuredVal,windowedTest,futuredTest\n",
    "\n",
    "###############################################################################################################\n",
    "# Choosing the parameters for the model. In the paper Input length is 100 and zeta changes. Shifting is something\n",
    "# that we added in order to ease the performance. \n",
    "\n",
    "INTERMEDIATE_LENGTH = 24\n",
    "INPUT_LENGTH        = 100\n",
    "zeta                = 24\n",
    "shifting            = 5\n",
    "windowedTrain,futuredTrain,windowedVal,futuredVal,windowedTest,futuredTest = trials_MARINA_Etth1(INPUT_LENGTH,zeta,shifting)\n",
    "\n",
    "###############################################################################################################\n",
    "# CREATE the MODEL\n",
    "# We created the model parametric in hence we will need to optimize the hyperparameters later.\n",
    "\n",
    "class InputSubBlock(nn.Module):\n",
    "    def __init__(self, n_layers, n_units):\n",
    "        super(InputSubBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(INPUT_LENGTH, n_units))  # First layer\n",
    "\n",
    "        for _ in range(n_layers - 1):  # Add remaining layers\n",
    "            self.layers.append(nn.Linear(n_units, n_units))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        return x\n",
    "\n",
    "class CascadeSubBlock(nn.Module):\n",
    "    def __init__(self, n_layers, n_units):\n",
    "        super(CascadeSubBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(n_units, n_units))  # First layer\n",
    "\n",
    "        for _ in range(n_layers - 1):  # Add remaining layers\n",
    "            self.layers.append(nn.Linear(n_units, n_units))\n",
    "\n",
    "        self.fc2 = nn.Linear(n_units, INPUT_LENGTH)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class ForecastingSubBlock(nn.Module):\n",
    "    def __init__(self, n_layers, n_units):\n",
    "        super(ForecastingSubBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(n_units, n_units))  # First layer\n",
    "\n",
    "        for _ in range(n_layers - 1):  # Add remaining layers\n",
    "            self.layers.append(nn.Linear(n_units, n_units))\n",
    "\n",
    "        self.fc2 = nn.Linear(n_units, INTERMEDIATE_LENGTH)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,input_n_layers, input_n_units, cascade_n_layers, cascade_n_units, forecast_n_layers, forecast_n_units):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.input = InputSubBlock(input_n_layers,input_n_units)\n",
    "        self.cascade = CascadeSubBlock(cascade_n_layers,cascade_n_units)\n",
    "        self.forecasting = ForecastingSubBlock(forecast_n_layers,forecast_n_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"1\",x.shape)\n",
    "        x = self.input(x)\n",
    "        #print(\"2s\",x.shape)\n",
    "        cascade = self.cascade(x)\n",
    "\n",
    "        forecast = self.forecasting(x)\n",
    "\n",
    "        return cascade , forecast\n",
    "class TemporalModule(nn.Module):\n",
    "    def __init__(self,input_n_layers, input_n_units, cascade_n_layers, cascade_n_units, forecast_n_layers, forecast_n_units):\n",
    "        super(TemporalModule, self).__init__()\n",
    "        self.mlp1 = MLPBlock(input_n_layers, input_n_units, cascade_n_layers, cascade_n_units, forecast_n_layers, forecast_n_units)\n",
    "        self.mlp2 = MLPBlock(input_n_layers, input_n_units, cascade_n_layers, cascade_n_units, forecast_n_layers, forecast_n_units)\n",
    "\n",
    "    def forward(self, input):\n",
    "        mlp1_out , forecast = self.mlp1(input)\n",
    "        new_input = mlp1_out - input\n",
    "        mlp2_out , forecast_2 = self.mlp2(new_input)\n",
    "        return (forecast + forecast_2)\n",
    "\n",
    "class SpatialModule(nn.Module):\n",
    "    def __init__(self, n_layers, n_units):\n",
    "        super(SpatialModule, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=INTERMEDIATE_LENGTH, num_heads=8, dropout=0.05)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(INTERMEDIATE_LENGTH, n_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.05),\n",
    "            nn.Linear(n_units, INTERMEDIATE_LENGTH)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.self_attention(x, x, x)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "class OutputModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OutputModule, self).__init__()\n",
    "        self.FFN = nn.Linear(INTERMEDIATE_LENGTH,zeta)\n",
    "    def forward(self, x):\n",
    "        x = self.FFN(x)\n",
    "        return x\n",
    "\n",
    "class modeMARINA(nn.Module):\n",
    "    def __init__(self, input_n_layers, input_n_units, cascade_n_layers, cascade_n_units, forecast_n_layers, forecast_n_units, spatial_n_layers, spatial_n_units):\n",
    "        super(modeMARINA, self).__init__()\n",
    "        self.temporal = TemporalModule(input_n_layers, input_n_units, cascade_n_layers, cascade_n_units, forecast_n_layers, forecast_n_units)\n",
    "        self.spatial = SpatialModule(spatial_n_layers, spatial_n_units)\n",
    "        self.output = OutputModule()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.temporal(x)\n",
    "        x = self.spatial(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "windowedTrainT = torch.tensor(windowedTrain).float()\n",
    "futuredTrainT  = torch.tensor(futuredTrain).float()\n",
    "windowedValT   = torch.tensor(windowedVal).float()\n",
    "futuredValT    = torch.tensor(futuredVal).float()\n",
    "windowedTestT  = torch.tensor(windowedTest).float()\n",
    "futuredTestT   = torch.tensor(futuredTest).float()\n",
    "\n",
    "###############################################################################################################\n",
    "# Training params and data loaders\n",
    "BATCH_SIZE          = 5       \n",
    "\n",
    "trainset            = MarinaDataset_Etth(windowedTrainT,futuredTrainT)\n",
    "trainloader         = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "validset            = MarinaDataset_Etth(windowedValT,futuredValT)\n",
    "validloader         = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "testset             = MarinaDataset_Etth(windowedTestT,futuredTestT)\n",
    "testloader          = torch.utils.data.DataLoader(testset, batch_size=1)\n",
    "\n",
    "###############################################################################################################\n",
    "# FORECASTING HYPERPARAMETERS\n",
    "n_units        = 200\n",
    "\n",
    "input_n_layers = 2  # Number of layers in InputSubBlock\n",
    "input_n_units = n_units  # Number of units in each layer of InputSubBlock\n",
    "\n",
    "cascade_n_layers = 2  # Number of layers in CascadeSubBlock\n",
    "cascade_n_units = n_units  # Number of units in each layer of CascadeSubBlock\n",
    "\n",
    "forecast_n_layers = 1  # Number of layers in ForecastingSubBlock\n",
    "forecast_n_units = n_units  # Number of units in each layer of ForecastingSubBlock\n",
    "\n",
    "spatial_n_layers = 1  # Number of layers in SpatialModule\n",
    "spatial_n_units = n_units  # Number of units in each layer of SpatialModule\n",
    "\n",
    "model = modeMARINA(input_n_layers, input_n_units, cascade_n_layers, cascade_n_units, forecast_n_layers, forecast_n_units, spatial_n_layers, spatial_n_units)\n",
    "\n",
    "###############################################################################################################\n",
    "# TRAIN THE MODEL FOR FORECASTING\n",
    "\n",
    "model           = model.to(device)\n",
    "lr              = 0.0002\n",
    "criterion       = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)  # Add weight decay parameter\n",
    "min_valid_loss  = np.inf\n",
    "epoch           = 30       # From the figure 7 in the paper.\n",
    "loss_history    = []\n",
    "valid_loss_history = []\n",
    "\n",
    "start_time = time.time() # Setting starting point for finding the execution time\n",
    "\n",
    "for epoch in range(epoch):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    loss_history.append(running_loss / len(trainloader))\n",
    "    valid_loss_history.append(valid_loss /len(validloader))\n",
    "    print(f'\\n\\nEpoch {epoch+1:2d} \\t Training Loss: {running_loss / len(trainloader):.6f} \\t Validation Loss: {valid_loss / len(validloader):.6f}')\n",
    "\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'\\t\\t Validation Loss Decreased ({min_valid_loss/len(validloader):.6f} --> {valid_loss/len(validloader):.6f})')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        # torch.save(model.state_dict(),'saved_model.pth')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total Execution Time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Plotting the loss history\n",
    "plt.plot(loss_history, label='Training Loss')\n",
    "plt.plot(valid_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss History')\n",
    "plt.grid(True)\n",
    "plt.legend()  # Add legend\n",
    "plt.show()\n",
    "\n",
    "# CALCULATE THE MSE \n",
    "model.eval()\n",
    "mse_score = calculate_mse(model, testloader)\n",
    "print(f\"MSE: {mse_score:.6f}\")\n",
    "\n",
    "# CALCULATE THE MAE \n",
    "mae_score = calculate_mae(model, testloader)\n",
    "print(f\"MAE: {mae_score:.6f}\")\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "# ANOMALY DETECTION\n",
    "# For SMAP Dataset\n",
    "LABELS_FILEPATH = \"./SMAP MSL/labeled_anomalies.csv\"\n",
    "TRAINSET_FILEPATH = \"./SMAP MSL/data/data/train\"\n",
    "TESTSET_FILEPATH = \"./SMAP MSL/data/data/test\"\n",
    "\n",
    "labels = pd.read_csv(LABELS_FILEPATH)\n",
    "labels.head(5)\n",
    "\n",
    "###############################################################################################################\n",
    "# Data loading \n",
    "# T-10 data is not in Labels but in Train Dataset. Therefore, T-10 will not be used for now.\n",
    "import os\n",
    "train_datas = {}\n",
    "test_datas = {}\n",
    "arranged_train_datas = {}\n",
    "arranged_test_datas = {}\n",
    "\n",
    "desiredSC      = 'SMAP'\n",
    "# Iterate directory\n",
    "for path in os.listdir(TRAINSET_FILEPATH):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(TRAINSET_FILEPATH, path)):\n",
    "        if path != 'T-10.npy':\n",
    "            SC_labels     = labels[labels['chan_id'] == path.replace('.npy','')]\n",
    "            SC            = SC_labels['spacecraft']\n",
    "            # print(\"SC_labels\",SC_labels)\n",
    "            # print(\"SC\",SC)\n",
    "            # print((SC.iloc[0]),path.replace('.npy',''))\n",
    "            SC_name       = SC.iloc[0]\n",
    "            if SC_name == desiredSC:\n",
    "                train_data = np.load(os.path.join(TRAINSET_FILEPATH, path))\n",
    "                train_datas[path] = train_data\n",
    "\n",
    "for path in os.listdir(TESTSET_FILEPATH):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(TESTSET_FILEPATH, path)):\n",
    "        if path != 'T-10.npy':\n",
    "            SC_labels     = labels[labels['chan_id'] == path.replace('.npy','')]\n",
    "            SC            = SC_labels['spacecraft']\n",
    "            SC_name       = SC.iloc[0]\n",
    "            if SC_name == desiredSC:       \n",
    "                test_data = np.load(os.path.join(TESTSET_FILEPATH, path))\n",
    "                test_datas[path] = test_data\n",
    "\n",
    "###############################################################################################################\n",
    "#4.1 data normalization \n",
    "#1 - training normalization \n",
    "wL                          = INPUT_LENGTH     # window length\n",
    "shift                       = shifting\n",
    "ALPHA                       = 0.1\n",
    "\n",
    "train_means = {}\n",
    "train_variances = {}\n",
    "windowedTrain = {}      # Dividing the data with windows\n",
    "futuredTrain = {}       # Creating arrays to be futured\n",
    "\n",
    "for train_data in train_datas:\n",
    "    #print(train_datas[train_data].shape)\n",
    "    D                                       = train_datas[train_data].shape[1]\n",
    "    iN                                      = train_datas[train_data].shape[0]\n",
    "\n",
    "    for D_ in range(D):\n",
    "        train_datas[train_data][:,D_]       = normalize_data(train_datas[train_data][:,D_],0,1)\n",
    "    #train_datas[train_data] = train_datas[train_data] * 2 - 1  # variance 1 olacak teyit et\n",
    "    \n",
    "    windowedTrain[train_data], futuredTrain[train_data] = windowed_Set(train_datas[train_data],wL,shift,zeta)\n",
    "\n",
    "print(\"Total set number is:\",len(windowedTrain),\n",
    "      \"\\n1 Set name:\",train_data,\n",
    "      \"\\nSet shape:\",train_datas[train_data].shape,\"[t,D]\",\n",
    "      \"\\nWindowed array shape:\",windowedTrain[train_data].shape,\"[D,i,w]\",\n",
    "      \"\\nFutured array shape:\",futuredTrain[train_data].shape,\"[D,i,zeta]\",\n",
    "      \"\\nOne sample mean:\", np.mean(train_datas[train_data]),\n",
    "      \"\\nOne sample dimension variance:\", np.std(train_datas[train_data][:,0]))\n",
    "      \n",
    "windowedTest    = {}\n",
    "futuredTest     = {}\n",
    "test_means      = {}\n",
    "test_variances  = {}\n",
    "test_std        = {}\n",
    "\n",
    "for test_data in test_datas:\n",
    "    windowedTest[test_data], futuredTest[test_data] = windowed_Set(test_datas[test_data],wL,shift,zeta)\n",
    "\n",
    "for test_data in test_datas:\n",
    "    #print(test_datas[test_data].shape)\n",
    "    D                               = windowedTest[test_data].shape[0]\n",
    "    iN                              = windowedTest[test_data].shape[1]\n",
    "\n",
    "    test_means[test_data]           = np.zeros((D,iN))\n",
    "    test_variances[test_data]       = np.zeros((D,iN))\n",
    "    test_std[test_data]             = np.zeros((D,iN))\n",
    "    test_means[test_data][:,0]      = 0\n",
    "    test_std[test_data][:,0]        = 0\n",
    "    test_variances[test_data][:,0]  = 1 \n",
    "\n",
    "    for iN_ in range(1,iN): \n",
    "        E                                = np.mean(windowedTest[test_data][:,iN_,:],axis = 1)\n",
    "        E2                               = np.mean(windowedTest[test_data][:,iN_,:]**2,axis = 1)    \n",
    "        test_means[test_data][:,iN_]     = (1-ALPHA)*test_means[test_data][:,iN_-1] + ALPHA*E \n",
    "        test_variances[test_data][:,iN_] = (1-ALPHA)*test_variances[test_data][:,iN_-1]+ALPHA*(E2-E**2)\n",
    "        test_std[test_data][:,iN_]       = np.sqrt(test_variances[test_data][:,iN_-1])\n",
    "        for D_ in range(D):\n",
    "            dynamicMean                       = test_means[test_data][D_,iN_-1]\n",
    "            dynamicVar                        = test_variances[test_data][D_,iN_-1]\n",
    "            dynamicStd                        = test_std[test_data][D_,iN_-1]\n",
    "            \n",
    "            windowedTest[test_data][D_,iN_,:] = normalize_data(windowedTest[test_data][D_,iN_,:],dynamicMean,dynamicStd)\n",
    "\n",
    "print(\"Window length:\", wL,\n",
    "      \"\\nShifting:\",shift,\n",
    "      \"\\nHorizon (zeta):\",zeta,\n",
    "      \"\\nTotal set number is:\",len(windowedTest),\n",
    "      \"\\n1 Set name:\",test_data,\n",
    "      \"\\nSet shape:\",test_datas[test_data].shape,\"[t,D]\",\n",
    "      \"\\nWindowed array shape:\",windowedTest[test_data].shape,\"[D,i,w]\",\n",
    "      \"\\nFutured array shape:\",futuredTest[test_data].shape,\"[D,i,zeta]\",\n",
    "      \"\\nTest means array shape:\",test_means[test_data].shape,\"[D,i]\",\n",
    "      \"\\nTest variance array shape:\",test_variances[test_data].shape,\"[D,i]\")\n",
    "\n",
    "\n",
    "# Stacking all data\n",
    "alltrainDataT      = []    # Create a list for training data\n",
    "allfuturedTrainT   = []    \n",
    "testSet          = []  \n",
    "test_futured       = [] \n",
    "\n",
    "testSetIdx         = []\n",
    "for i,name in enumerate(windowedTrain.keys()):\n",
    "    for j in range(windowedTrain[name].shape[1]):                                 # Looping over T dimension for stacking                              \n",
    "        alltrainDataT.append(torch.tensor(windowedTrain[name][:,0,:]).float())   # Swapping the dimensions accordingly as (T,D,W).\n",
    "        allfuturedTrainT.append(torch.tensor(futuredTrain[name][:,0,:]).float()) # Swapping the dimensions accordingly as (T,D,W).\n",
    "      \n",
    "for i,name in enumerate(windowedTest.keys()):\n",
    "    for j in range(windowedTest[name].shape[1]):                                 # Looping over T dimension for stacking                              \n",
    "        testSet.append(torch.tensor(windowedTest[name][:,0,:]).float())   # Swapping the dimensions accordingly as (T,D,W).\n",
    "        test_futured.append(torch.tensor(futuredTest[name][:,0,:]).float()) # Swapping the dimensions accordingly as (T,D,W).\n",
    "        testSetIdx.append([name, wL + zeta + shift*j - 1])\n",
    "\n",
    "    #print(windowedTrain[name].shape)\n",
    "print(\"List length of whole trainDataT:\",len(alltrainDataT),\", and shape of input\",alltrainDataT[1].shape,\n",
    "      \"\\nLength of whole futured (targets) list:\",len(allfuturedTrainT),\", and shape of output\",allfuturedTrainT[1].shape)\n",
    "print(\"\\nList length of whole testSet:\",len(testSet),\", and shape of input\",testSet[1].shape,\n",
    "      \"\\nLength of whole futured test (targets) list:\",len(test_futured),\", and shape of output\",test_futured[1].shape)\n",
    "\n",
    "\n",
    "# Generate random indices for the validation set\n",
    "valP            = 0.2 # Select 20% of the indices for validation\n",
    "val_indices = random.sample(range(len(alltrainDataT)), k=int(len(alltrainDataT) * valP))  \n",
    "# Split the data into training and validation sets\n",
    "trainSet, validSet, train_futured, val_futured = [], [], [], []\n",
    "for i in range(len(alltrainDataT)):\n",
    "    if i in val_indices:\n",
    "        validSet.append(alltrainDataT[i])\n",
    "        val_futured.append(allfuturedTrainT[i])\n",
    "    else:\n",
    "        trainSet.append(alltrainDataT[i])\n",
    "        train_futured.append(allfuturedTrainT[i])\n",
    "\n",
    "print(\"Training data and futured:\")\n",
    "print(len(trainSet),len(train_futured))\n",
    "print(\"Validation data and futured:\")\n",
    "print(len(validSet),len(val_futured))\n",
    "print(\"Test data and futured:\")\n",
    "print(len(testSet),len(test_futured))\n",
    "\n",
    "# Training params\n",
    "BATCH_SIZE                  = 128\n",
    "lr                          = 0.005\n",
    "\n",
    "trainset            = MarinaDataset_Smap(trainSet,train_futured)\n",
    "trainloader         = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,shuffle=True)      \n",
    " \n",
    "validset            = MarinaDataset_Smap(validSet,val_futured)\n",
    "validloader         = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "testset            = MarinaDataset_Smap(testSet,test_futured)\n",
    "testloader         = torch.utils.data.DataLoader(testset, batch_size=1)\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "# ANOMALY HYPERPARAMETERS\n",
    "n_units        = 50\n",
    "\n",
    "input_n_layers = 4  # Number of layers in InputSubBlock\n",
    "input_n_units = n_units  # Number of units in each layer of InputSubBlock\n",
    "\n",
    "cascade_n_layers = 2  # Number of layers in CascadeSubBlock\n",
    "cascade_n_units = n_units  # Number of units in each layer of CascadeSubBlock\n",
    "\n",
    "forecast_n_layers = 2  # Number of layers in ForecastingSubBlock\n",
    "forecast_n_units = n_units  # Number of units in each layer of ForecastingSubBlock\n",
    "\n",
    "spatial_n_layers = 1  # Number of layers in SpatialModule\n",
    "spatial_n_units = n_units  # Number of units in each layer of SpatialModule\n",
    "\n",
    "model_Anomaly = modeMARINA(input_n_layers, input_n_units, cascade_n_layers, cascade_n_units, forecast_n_layers, forecast_n_units, spatial_n_layers, spatial_n_units)\n",
    "\n",
    "###############################################################################################################\n",
    "# TRAINING MODEL FOR ANOMALY DETECTION\n",
    "\n",
    "model           = model_Anomaly\n",
    "model           = model.to(device)\n",
    "criterion       = nn.MSELoss()\n",
    "optimizer       = optim.Adam(model.parameters(), lr=lr)\n",
    "min_valid_loss  = np.inf\n",
    "epoch           = 30       # From the figure 7 in the paper.\n",
    "loss_history    = []\n",
    "\n",
    "start_time = time.time() # Setting starting point for finding the execution time\n",
    "\n",
    "for epoch in range(epoch):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    loss_history.append(running_loss / len(trainloader))\n",
    "    print(f'Epoch {epoch+1:2d} \\t Training Loss: {running_loss / len(trainloader):.6f} \\t Validation Loss: {valid_loss / len(validloader):.6f}')\n",
    "\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'\\t\\t Validation Loss Decreased ({min_valid_loss/len(validloader):.6f} --> {valid_loss/len(validloader):.6f})')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        # torch.save(model.state_dict(),'saved_model.pth')\n",
    "    \n",
    "print('Finished Training')\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total Execution Time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Plotting the loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss History')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "mse_score = calculate_mse(model, testloader)\n",
    "print(f\"MSE: {mse_score:.6f}\")\n",
    "\n",
    "#anomalies = {key: default_value for key in keys}\n",
    "anomalies = {}\n",
    "i = 0\n",
    "ii = 0\n",
    "threshold = 0.85\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        if ii < 8640:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            #compare output with labels using frobenius norm\n",
    "            frob_error  =  torch.sqrt(torch.abs(torch.sum(outputs ** 2 - labels ** 2)))\n",
    "            #this if else is for catching the beginning and end of the sequences\n",
    "            if( frob_error > threshold ):\n",
    "                [keyN, valueN] =  testSetIdx[ii]\n",
    "                if keyN not in anomalies:\n",
    "                    anomalies[keyN] = []\n",
    "                anomalies[keyN].append(valueN)\n",
    "            ii += 1                                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b369e7963f7abcd480fad95d5beb7b146f8ec315fde1b361e4f21d5ea962be01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
