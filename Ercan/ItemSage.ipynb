{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465fd4ce-0aa1-450c-a885-4562d7bb5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, hashlib, torch\n",
    "from nltk import ngrams, word_tokenize\n",
    "\n",
    "#nltk.download(\"punkt\")\n",
    "unigramCount = 200000\n",
    "bigramCount = 1000000\n",
    "trigramCount = 64000\n",
    "def create_string_embeddings(text):\n",
    "    bow = []\n",
    "\n",
    "    for s in text:\n",
    "        # word unigram 200.000\n",
    "        print(\"first text is::::::\", s)\n",
    "        uni = list(ngrams( word_tokenize(s), 1 ) )\n",
    "        unigrams = [\"\".join(i) for i in uni ]\n",
    "        unigramN = [int(hashlib.sha1(p.encode('utf-8')).hexdigest(), 16) % (unigramCount-1) + 1 for p in unigrams]\n",
    "        #print(\"======word unigrams:\", unigrams, unigramN)\n",
    "\n",
    "        # word bigrams 1.000.000\n",
    "        bis = list( ngrams( word_tokenize(s), 2) )\n",
    "        bigrams = [\"\".join( i[0]+\" \"+i[1] ) for i in bis]\n",
    "        bigramN = [int(hashlib.sha1(p.encode('utf-8')).hexdigest(), 16) % (bigramCount-1) + unigramCount + 1 for p in bigrams]\n",
    "        #print(\"word bigrams:\", bigrams, bigramN )\n",
    "\n",
    "        #character trigrams 64.000\n",
    "        tris = list(ngrams(s,3))\n",
    "        trigrams = [\"\".join(i) for i in tris]\n",
    "        trigramN = [int(hashlib.sha1(p.encode('utf-8')).hexdigest(), 16) % (trigramCount-1) + bigramCount + unigramCount + 1 for p in trigrams]\n",
    "\n",
    "        #print(\"character trigrams:\", trigrams, trigramN )\n",
    "        cur = unigramN + bigramN + trigramN\n",
    "        cur = cur[0:99]\n",
    "        bow.append( cur + [0]*(100-len(cur)))\n",
    "        #print(\"bow is:\", bow)\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9124c9-957b-4bcc-9cac-e38e48bfe657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mmh3\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, emsize, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.ntokens = ntoken\n",
    "        \n",
    "        self.E = nn.Embedding(ntoken,256,padding_idx=0)\n",
    "        self.W = nn.Embedding(unigramCount+bigramCount+trigramCount, 2)\n",
    "        self.clsToken = torch.randn((1,512))\n",
    "        \n",
    "        self.textLinear = nn.Linear(100*256,512)\n",
    "        self.imageLinear = nn.Linear(2048,512)\n",
    "        self.clsLinear = nn.Linear(512,512)\n",
    "        \n",
    "        encoder_layers = TransformerEncoderLayer(emsize, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, 256))\n",
    "\n",
    "        #self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, txtsrc, imgsrc):        \n",
    "        out = create_string_embeddings(txtsrc)\n",
    "        out = torch.Tensor(out)\n",
    "        \n",
    "        # Get 2 different hash index from W for each token ID & Get 2 importance weight from E for each token ID\n",
    "        txtEmbeddings = torch.Tensor()\n",
    "        for ems in out: \n",
    "            stringEmbedding = torch.Tensor([])\n",
    "            for hvalue in ems:\n",
    "                if hvalue != 0:\n",
    "                    hash1index = mmh3.hash(str(hvalue), seed=1) % self.ntokens\n",
    "                    hash2index = mmh3.hash(str(hvalue), seed=2) % self.ntokens\n",
    "                    hashes = self.E( torch.IntTensor([hash1index, hash2index]) )\n",
    "                    \n",
    "                    importanceWeights = self.W( hvalue.int() )\n",
    "                    #print(\"here is the hashes:\", hashes, \" importance weights: \", importanceWeights)\n",
    "                    #print(\"final embeddings are:\", (hashes[0]*importanceWeights[0] + hashes[1]*importanceWeights[1])/2 )\n",
    "                    finalEmbedding = (hashes[0]*importanceWeights[0] + hashes[1]*importanceWeights[1])/2\n",
    "                    stringEmbedding = torch.cat([stringEmbedding, finalEmbedding])\n",
    "                    #print(\"string Embedding is: \", stringEmbedding.shape )\n",
    "                else:\n",
    "                    stringEmbedding = torch.cat([stringEmbedding, torch.zeros([256])])\n",
    "            #print(\"Last string Embedding is: \", stringEmbedding.unsqueeze(0).shape )\n",
    "            txtEmbeddings = torch.cat([txtEmbeddings, stringEmbedding.unsqueeze(0)])\n",
    "        \n",
    "        \n",
    "        # pass the hashes to textLinear\n",
    "        txtEmbeddings = self.textLinear(txtEmbeddings)\n",
    "        \n",
    "        # pass the image embeddings to imgLinear\n",
    "        imgEmbeddings = self.imageLinear(imgsrc)\n",
    "        \n",
    "        # get cls token\n",
    "        clsToken = self.clsLinear(self.clsToken)\n",
    "        \n",
    "        allEmbeddings = torch.cat([clsToken, txtEmbeddings, imgEmbeddings])\n",
    "        print(\"All Embedding shape: \", allEmbeddings.shape)\n",
    "        \n",
    "        output = self.transformer_encoder(allEmbeddings)\n",
    "        print(output[0].shape)\n",
    "        output = self.seq( output[0] )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db1bfad-aec5-4144-ac01-18cd60e74b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ResNetModel: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'key1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m imgEmbeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m---> 28\u001b[0m     imread \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     processedImage \u001b[38;5;241m=\u001b[39m processor(imread,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     30\u001b[0m     sonuc \u001b[38;5;241m=\u001b[39m resnetModel(processedImage)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/matplotlib/pyplot.py:2168\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2166\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimread)\n\u001b[1;32m   2167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(fname, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 2168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/matplotlib/image.py:1561\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parse\u001b[38;5;241m.\u001b[39murlparse(fname)\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;66;03m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease open the URL for reading and pass the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult to Pillow, e.g. with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1560\u001b[0m         )\n\u001b[0;32m-> 1561\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimg_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[1;32m   1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1563\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m             pil_to_array(image))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3227\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'key1.jpg'"
     ]
    }
   ],
   "source": [
    "from transformers import ResNetModel, AutoProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ntokens = 100000 # the size of vocabulary\n",
    "emsize = 512 # embedding dimension\n",
    "nhid = 512 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
    "\n",
    "# GET INPUT\n",
    "\n",
    "# Create embeddings for strings\n",
    "txtsrc = [\"NPET K10 Wired Gaming Keyboard, LED Backlit, Spill-Resistant Design, Multimedia Keys\"]\n",
    "txtsrc.append(\"Gaming, Membrane, Multimedia, Mechanical\")\n",
    "txtsrc.append(\"10-Zone RGB Lighting: With 16.8 million colors and a suite\")\n",
    "\n",
    "# Get embeddings for images\n",
    "model_name = \"microsoft/resnet-50\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "resnetModel = ResNetModel.from_pretrained(model_name)\n",
    "images = [\"samples/key1.jpg\", \"samples/key2.jpg\", \"samples/key3.jpg\", \"samples/key4.jpg\", \"samples/key5.jpg\"]\n",
    "\n",
    "imgEmbeddings = torch.Tensor()\n",
    "for img in images:\n",
    "    imread = plt.imread(img)\n",
    "    processedImage = processor(imread,return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    sonuc = resnetModel(processedImage)\n",
    "    imgEmbeddings = torch.cat( [imgEmbeddings, sonuc.pooler_output.squeeze(-1).squeeze(-1)] )\n",
    "\n",
    "print(imgEmbeddings)\n",
    "\n",
    "model(txtsrc, imgEmbeddings).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36246ab2-de5f-42c7-9563-3ce30ece80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.cat((p, torch.Tensor([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534b1c0-3914-4e10-9ed2-d95461f8a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((p, torch.Tensor([2,4,5]) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899cb80c-b68b-4bfa-a9dd-c17c801fb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = torch.tensor([])\n",
    "x = torch.unsqueeze( torch.randn( 7), dim=0 )\n",
    "y = torch.cat([empty, x])\n",
    "y = torch.cat([y, x]).unsqueeze(0)\n",
    "print(y )\n",
    "\n",
    "empty2 = torch.Tensor()\n",
    "z = torch.cat([empty2, y])\n",
    "z = torch.cat([z, y])\n",
    "print(\"z is:\",z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384b7f7-1fbb-4561-9c6d-e9e9eeb7c466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
