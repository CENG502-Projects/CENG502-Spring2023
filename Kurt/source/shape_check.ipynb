{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input [batch x frames x input_size]\n",
    "\n",
    "input = torch.zeros(100, 400, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial embedd \n",
    "in_channels = 6\n",
    "out_channels = 6\n",
    "spatial_embedder = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm1d(out_channels),\n",
    "            torch.nn.Linear(400, 400)\n",
    "            ) #TODO linear layers (batch ve linear layerin cogul eki herhalde 1 tane var)?, sizelari salladim\n",
    "\n",
    "conv1d = torch.nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "input_t = input.transpose(1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# channel mantiken x,y,z axisleri olan 6 dimensionli taraf olmali\n",
    "linear = torch.nn.Linear(400, 400)\n",
    "\n",
    "out = spatial_embedder(input_t)\n",
    "\n",
    "out.shape[2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 400, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_temporal import BilinearLSTMSeqNetwork\n",
    "input = torch.zeros(100, 400, 6).to(0)\n",
    "a = BilinearLSTMSeqNetwork(6, 6, 100, 0).to(0)\n",
    "a(input).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model_temporal import SpatialEncoderLayer\n",
    "s = SpatialEncoderLayer(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([100, 6, 400])\n",
      "conv11_0 torch.Size([100, 6, 400])\n",
      "batch norm 0 torch.Size([100, 6, 400])\n",
      "relu torch.Size([100, 6, 400])\n",
      "conv33 torch.Size([100, 6, 400])\n",
      "concat torch.Size([100, 12, 400])\n",
      "weight torch.Size([100, 6, 400])\n",
      "relu torch.Size([100, 6, 400])\n",
      "c2 torch.Size([100, 6, 400])\n",
      "transpose torch.Size([100, 400, 6])\n",
      "attention torch.Size([100, 400, 6])\n",
      "re transpose torch.Size([100, 6, 400])\n",
      "conv11_3 torch.Size([100, 6, 400])\n",
      "bn 1 torch.Size([100, 6, 400])\n",
      "relu torch.Size([100, 6, 400])\n",
      "global self attention torch.Size([100, 400, 6])\n",
      "conv11 torch.Size([100, 6, 400])\n",
      "torch.Size([100, 6, 400])\n",
      "layer shape torch.Size([6, 6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"input\", input_t.shape)\n",
    "a1 = spatial_embedder(input_t)\n",
    "print(\"conv11_0\", s.conv11_0(a1).shape)\n",
    "\n",
    "a2 = s.conv11_0(a1)\n",
    "print(\"batch norm 0\", s.batch_norm_0(a1).shape)\n",
    "a3 = s.batch_norm_0(a2)\n",
    "print(\"relu\", s.relu(a3).shape)\n",
    "a4  = s.relu(a3)\n",
    "print(\"conv33\", s.conv33(a4).shape)\n",
    "c1 = s.conv33(a4)\n",
    "print(\"concat\", torch.concat([c1, a2], 1).shape)\n",
    "t = torch.concat([c1, a2], 1)\n",
    "print(\"weight\", s.conv11_2(t).shape)\n",
    "t2 = s.conv11_2(t)\n",
    "print('relu', s.relu(t2).shape)\n",
    "t3 = s.relu(t2)\n",
    "v = s.conv11_1(input_t)\n",
    "print(\"c2\", torch.mul(t3, v).shape)\n",
    "c2 = torch.mul(t3, v)\n",
    "print(\"transpose\", c2.transpose(1,2).contiguous().shape)\n",
    "c3 = c2.transpose(1, 2).contiguous()\n",
    "c4 = c1.transpose(1, 2).contiguous()\n",
    "y, _ = s.attention(c4, c3, c3)\n",
    "print(\"attention\",y.shape)\n",
    "print(\"re transpose\", y.transpose(1,2).shape)\n",
    "yt = y.transpose(1,2)\n",
    "print(\"conv11_3\", s.conv11_3(yt).shape)\n",
    "z = s.conv11_3(yt)\n",
    "print('bn 1', s.batch_norm_1(z).shape)\n",
    "z1 = s.batch_norm_1(z)\n",
    "print('relu', s.relu(z1).shape)\n",
    "z2 = s.relu(z1)\n",
    "z2t = z2.transpose(1,2).contiguous()\n",
    "y2, _ = s.global_self_attention(z2t, z2t, z2t)\n",
    "print(\"global self attention\", y2.shape)\n",
    "y2t = y2.transpose(1,2).contiguous()\n",
    "print(\"conv11\", s.conv11_4(y2t).shape)\n",
    "y3 = s.conv11_4(y2t)\n",
    "y4 = s.batch_norm_2(y3)\n",
    "y5 = y4 + input_t\n",
    "y6 = s.relu(y5)\n",
    "print(y6.shape)\n",
    "print(\"layer shape\", s.conv11_0.weight.data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporal embedding  torch.Size([100, 400, 6])\n",
      "zz torch.Size([100, 400, 6]) y6 torch.Size([100, 6, 400]) y6_t torch.Size([100, 400, 6])\n",
      "temporal decoder torch.Size([100, 400, 6])\n",
      "zz_t torch.Size([100, 6, 400]) y6 torch.Size([100, 6, 400])\n",
      "temporal decoder torch.Size([100, 6, 400])\n"
     ]
    }
   ],
   "source": [
    "# temporal branch\n",
    "input2 = torch.zeros(100, 400, 6).to(0)\n",
    "temp = torch.nn.TransformerDecoderLayer(6, 2)\n",
    "temp2 = torch.nn.TransformerDecoder(temp, 1).to(0)\n",
    "p = BilinearLSTMSeqNetwork(6, 6, 100, 0).to(0)\n",
    "\n",
    "print(\"temporal embedding \", p(input2).shape)\n",
    "zz = p(input2)\n",
    "y6_t = y6.transpose(1, 2).contiguous()\n",
    "zz_t = zz.transpose(1, 2).contiguous()\n",
    "print(\"zz\", zz.shape, \"y6\", y6.shape, \"y6_t\", y6_t.shape)\n",
    "print(\"temporal decoder\", temp2(zz, y6_t.to(0)).shape)\n",
    "\n",
    "\n",
    "temp3 = torch.nn.TransformerDecoderLayer(400, 8)\n",
    "temp4 = torch.nn.TransformerDecoder(temp3, 1).to(0)\n",
    "print(\"zz_t\", zz_t.shape, \"y6\", y6.shape)\n",
    "print(\"temporal decoder\", temp4(zz_t, y6.to(0)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 400, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = temp4(zz_t, y6.to(0))\n",
    "ht = h.transpose(1,2).contiguous()\n",
    "\n",
    "from model_temporal import CTINNetwork\n",
    "\n",
    "ctin = CTINNetwork(6, 2, 100, 400, 1, 4, 0).to(0)\n",
    "\n",
    "ctin.linear_linear_vel(ht).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([100, 400, 6]), out1 shape torch.Size([100, 400, 2]), out2 shape torch.Size([100, 400, 2])\n"
     ]
    }
   ],
   "source": [
    "out1, out2  = ctin(input)\n",
    "print(f\"input shape {input.shape}, out1 shape {out1.shape}, out2 shape {out2.shape}\")\n",
    "gt = torch.ones(100, 400, 2).to(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 399, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pos = torch.cumsum(out1[:, 1:, ], 1)\n",
    "gt_pos = torch.cumsum(gt[:, 1:, ], 1)\n",
    "pred_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lvp shape torch.Size([])\n",
      "cumsum torch.Size([100, 399, 2])\n",
      "mean torch.Size([])\n",
      "lv torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "mse_loss = torch.nn.MSELoss()\n",
    "lv_p = mse_loss(pred_pos, gt_pos)\n",
    "print(\"lvp shape\", lv_p.shape)\n",
    "diffs = torch.square(gt[:, 1:, ] - out1[:, 1:, ])\n",
    "lv_e = torch.cumsum(diffs, 1)\n",
    "print(\"cumsum\",lv_e.shape)\n",
    "lv_e = torch.mean(lv_e)\n",
    "print(\"mean\", lv_e.shape)\n",
    "lv = lv_p + lv_e\n",
    "print(\"lv\", lv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff torch.Size([100, 400, 2])\n",
      "p1 torch.Size([100, 400, 2, 2])\n",
      "p2 torch.Size([100, 400, 1, 2])\n",
      "lc torch.Size([100, 400, 1, 2])\n",
      "torch.Size([100, 400, 1, 1])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "diff = torch.square(gt - out1)\n",
    "p1 = (1 / (torch.diag_embed(out2) + 1e-5))\n",
    "print(\"diff\", diff.shape)\n",
    "print(\"p1\",p1.shape)\n",
    "p2 = diff[:, :, None, :]\n",
    "print(\"p2\", p2.shape)\n",
    "lc = p2 @ p1\n",
    "print(\"lc\",lc.shape)\n",
    "#lc += 0.5 * torch.log(out2[:, :1, 0] * out2[:, :1, 1]) \n",
    "lc2  = lc @ diff[:, :, :, None]\n",
    "print(lc2.shape)\n",
    "lcf = torch.mean(lc2)\n",
    "print(lcf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff torch.Size([2, 1])\n",
      "tensor([[17.]])\n",
      "tensor([[17.]])\n",
      "tensor([[17.]])\n",
      "tensor([[1.],\n",
      "        [2.]]) tensor([[1.0000],\n",
      "        [0.5000]])\n",
      "torch.Size([2, 2, 10, 10])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([10, 10, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[3], [2]])\n",
    "print(\"diff\", a.shape)\n",
    "b = torch.Tensor([[1, 0], [0, 2]])\n",
    "c = torch.Tensor([[1], [2]])\n",
    "print(a.transpose(0, 1) @ b @ a)\n",
    "print((a * c).transpose(0, 1) @ a)\n",
    "print(torch.square(a).transpose(0, 1) @ c)\n",
    "print(c , 1/c)\n",
    "\n",
    "kk = torch.zeros(2, 1, 10, 10)\n",
    "tt = torch.zeros(2, 2, 10, 10)\n",
    "print((tt @ kk).shape)\n",
    "\n",
    "aa = torch.zeros(2, 1)\n",
    "bb = torch.zeros(2, 2)\n",
    "print((bb @ aa).shape)\n",
    "\n",
    "yy = torch.zeros(10, 10, 1, 2)\n",
    "zz = torch.zeros(10, 10, 2, 2)\n",
    "print((yy @ zz).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn(10, 100, 2)\n",
    "b = torch.randn(10, 100, 2, 2)\n",
    "\n",
    "# Perform element-wise matrix multiplication using torch.einsum\n",
    "result = torch.einsum('...ij,...ijk->...ik', a, b)\n",
    "\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(10, 100, 2)\n",
    "b = torch.randn(10, 100, 2, 2)\n",
    "\n",
    "result = torch.matmul(a[:, :, None, :], b[:, :, :, :])\n",
    "\n",
    "print(result.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ronin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
