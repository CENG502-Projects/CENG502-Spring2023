{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS\n",
    "* BELOW I HAVE TESTED IF TRANSFORMS CALCULATE THE RANDOM PROBABILITY SEPERATELY FOR EVERY IMAGE IN A\n",
    "* ANSWER -> IT DOES NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = T.RandomResizedCrop((image_size, image_size))\n",
    "import torchvision\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "params= t.get_params(img, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333)) # returns top: int, left: int, height: int, width\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "t_color = T.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
    "params = t_color.get_params((0.2, 1.8), (0.2, 1.8), (0.2, 1.8), (-0.2, 0.2))\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "class RandomHorizontalFlip(torch.nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        prob = torch.rand(1)\n",
    "        print(prob)\n",
    "        if prob < self.p:\n",
    "            return F.hflip(img)\n",
    "        return img\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "a = RandomHorizontalFlip()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "print(img.shape)\n",
    "_, height, width = F.get_dimensions(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "def random_hflip(img, p):\n",
    "    flip_bool = torch.rand(1) < p\n",
    "    return (F.hflip(img),flip_bool) if flip_bool else (img, flip_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "conv = torch.nn.Conv2d(3, 4, 2)\n",
    "\n",
    "at = torch.ones((3,4,3,60,60))\n",
    "print(at.size())\n",
    "bt = at.reshape(-1,*at.size()[-3:])\n",
    "print(bt.size())\n",
    "conv(bt).shape\n",
    "instance = F.interpolate(bt, size=(96, 96), mode=\"bicubic\") # resize instance to 96x96\n",
    "instance.shape\n",
    "torch.stack([bt,bt]).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming O and T are your input matrices of size KxM and KxN respectively.\n",
    "# Horizontal stack of column vectors of lenght K (K is number of instances)\n",
    "O_matrix = torch.tensor([[3,5,6],[4,12,8]], dtype=torch.float)# (M is number of features) \n",
    "T_matrix = torch.tensor([[1,2,3],[4,2,5]], dtype=torch.float)# (N is number of features)\n",
    "\n",
    "# Step 1: Compute DotProductMatrix\n",
    "DotProductMatrix = torch.mm(O_matrix.t(), T_matrix)\n",
    "\n",
    "# Step 2: Compute Norm matrices\n",
    "NormMatrixO = torch.norm(O_matrix, dim=0, keepdim=True)\n",
    "NormMatrixT = torch.norm(T_matrix, dim=0, keepdim=True)\n",
    "\n",
    "# Step 3: Compute C\n",
    "C = 1 - (DotProductMatrix / (NormMatrixO.t() * NormMatrixT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming O and T are your input matrices of size KxM and KxN respectively.\n",
    "# Horizontal stack of column vectors of lenght K (K is number of instances)\n",
    "O = torch.tensor([[3,5,6],[4,12,8]], dtype=torch.float)# (M is number of features) \n",
    "T = torch.tensor([[1,2,3],[4,2,5]], dtype=torch.float)# (N is number of features)\n",
    "\n",
    "# Step 1: Compute DotProductMatrix\n",
    "DotProductMatrix = torch.mm(O.t(), T)\n",
    "O.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute Norm matrices, output will be a horizontal vector.\n",
    "\n",
    "NormMatrixO = torch.norm(O, dim=0, keepdim=True) # find norm of columns\n",
    "NormMatrixT = torch.norm(T, dim=0, keepdim=True)\n",
    "NormMatrixO.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormMatrixT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO check if this (NormMatrixO.t() * NormMatrixT) matches the target indices.\n",
    "# Step 3: Compute C\n",
    "C = 1 - (DotProductMatrix / (NormMatrixO.t() * NormMatrixT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test concatentation   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.ops\n",
    "\n",
    "def get_concatenated_instances(img, overlapping_boxes):\n",
    "    # img shape should be (b, c, h, w) - batch, channels, height, width\n",
    "    # overlapping_boxes shape should be (b, n, 4) - batch, number of boxes, coordinates (x1, y1, x2, y2)\n",
    "\n",
    "    # Number of boxes per image\n",
    "    num_boxes = overlapping_boxes.size(1)\n",
    "    \n",
    "    # Create batch indices to be concatenated with boxes -> (batch_size*K), each box will have an index (showing where it belongs)\n",
    "    batch_indices = torch.arange(img.size(0), dtype=torch.float32).view(-1, 1).repeat(1, num_boxes).view(-1, 1)\n",
    "    print(batch_indices.shape)\n",
    "    print(batch_indices)\n",
    "    \n",
    "    # Reshape boxes for roi_align\n",
    "    boxes = overlapping_boxes.view(-1, 4) # Collect total number of boxes in the first dim (batch_size*K)\n",
    "    \n",
    "    print(boxes.shape)\n",
    "    # Concatenate batch indices with boxes, index shows which image in a batch each box belongs\n",
    "    boxes_with_indices = torch.cat([batch_indices, boxes], dim=1)\n",
    "    \n",
    "    # Crop and resize using roi_align\n",
    "    output_size = (96, 96)\n",
    "    instances = torchvision.ops.roi_align(img, boxes_with_indices, output_size)\n",
    "    \n",
    "    # Now instances tensor has shape (b * n, c, 96, 96), TODO you can reshape it if needed\n",
    "    return instances\n",
    "\n",
    "# Example usage\n",
    "b, c, h, w = 3, 3, 256, 256\n",
    "img_batch = torch.randn(b, c, h, w)\n",
    "overlapping_boxes_batch = torch.tensor([[[50, 50, 100, 100], [30, 30, 80, 80]], [[10, 10, 60, 60], [20, 20, 70, 70]], [[40, 40, 110, 110], [50, 50, 120, 120]]])\n",
    "\n",
    "# instances_batch = get_concatenated_instances(img_batch, overlapping_boxes_batch)\n",
    "# print(instances_batch.shape)  # Should be (b * n, c, 96, 96)\n",
    "img = torch.randn(1, c, h, w)\n",
    "overlapping_boxes = torch.tensor([[[50, 50, 100, 100], [30, 30, 80, 80]]])\n",
    "instances = get_concatenated_instances(img, overlapping_boxes)\n",
    "print(instances.shape)  # Should be (b * n, c, 96, 96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([*overlapping_boxes.shape[0:2], *instances.shape[-3:]])\n",
    "instances = instances.reshape(*overlapping_boxes.shape[0:2], *instances.shape[-3:])\n",
    "torch.concat(instances, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "T_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "dot_product_matrix = torch.matmul(O_matrix, T_matrix.transpose(1, 2))  # (batch_size, number of features, number of features)\n",
    "norm_vector_O = torch.norm(O_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "norm_vector_T = torch.norm(T_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "print(norm_vector_O.shape)\n",
    "print(norm_vector_T.shape)\n",
    "torch.matmul(norm_vector_O, norm_vector_T.transpose(1, 2)).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SINKHORN DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute dot_product_matrix\n",
    "online_pred_avg = torch.randn(3, 1, 5)  # (batch_size, instance numbers K, number of features)\n",
    "O_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "target_proj_avg = torch.randn(3, 1, 5)  # (batch_size, instance numbers K, number of features)\n",
    "T_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "dot_product_matrix = torch.matmul(O_matrix, T_matrix.transpose(1, 2))  # (batch_size, instance numbers K, instance numbers K)\n",
    "# Step 2: Compute Norm matrices\n",
    "norm_vector_O = torch.norm(O_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "norm_vector_T = torch.norm(T_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "# Step 3: Compute C\n",
    "ot_cosine_similarity_matrix = (dot_product_matrix / torch.matmul(norm_vector_O, norm_vector_T.transpose(1, 2)))\n",
    "cost_matrix = 1 - ot_cosine_similarity_matrix\n",
    "a_vector = torch.nn.functional.relu(torch.matmul(T_matrix, online_pred_avg.transpose(1, 2))) # (batch_size, instance numbers K, 1)\n",
    "print(a_vector.values.shape)\n",
    "print(torch.matmul(O_matrix, target_proj_avg.transpose(1, 2)).shape)\n",
    "b_vector = torch.nn.functional.relu(torch.matmul(O_matrix, target_proj_avg.transpose(1, 2))) # (batch_size, instance numbers K, 1)\n",
    "print(b_vector.shape)\n",
    "print(b_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = torch.randn(1, 4, 1)  # (batch_size, instance numbers K, number of features)\n",
    "at.squeeze(dim=-1).shape # demander a (batch_size, instance numbers K, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, instance numbers K, instance numbers K)\n",
    "optimal_plan_matrix = torch.randn(5, 4, 4)  # (batch_size, instance numbers K, number of features)\n",
    "ot_cosine_similarity_matrix = torch.randn(5, 4, 4)  # (batch_size, instance numbers K, number of features)\n",
    "torch.sum(-torch.mul(optimal_plan_matrix,ot_cosine_similarity_matrix), dim=(-2,-1)).mean() # Forces similar instance representations to be close to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "image_size = 224\n",
    "torch.randn(2, 3, image_size, image_size, device=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "import random\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/data_ops/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "print(img.dtype)\n",
    "img[0,0,0] = 255\n",
    "torchvision.transforms.functional.adjust_brightness(img, brightness_factor=0.2)\n",
    "torchvision.transforms.functional.adjust_contrast(img, 0.2)\n",
    "torchvision.transforms.functional.adjust_saturation(img, 0.2)\n",
    "\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        return x if random.random() > self.p else self.fn(x)\n",
    "\n",
    "from torchvision import transforms as T\n",
    "import torch\n",
    "img = torch.rand(size=(1, 3, 224, 224))*255\n",
    "# img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.dtype)\n",
    "print((img).max())\n",
    "col_jit = RandomApply(T.ColorJitter(0.4, 0.4, 0.2, 0.1), p = 0.8)\n",
    "print(col_jit(img).max())\n",
    "print(T.ColorJitter(0.4, 0.4, 0.2, 0.1)(img).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.transforms.functional.adjust_brightness(img/255, brightness_factor=0.2)\n",
    "(img/255).dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(size=(3, 1, 3, 255, 255)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "im = cv2.imread(\"/home/kuartis-dgx1/utku/UniVIP/data_ops/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/data_ops/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "torchvision.transforms.functional.adjust_brightness(img, brightness_factor=0.2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK NUMBER OF SMALL IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import multiprocessing\n",
    "\n",
    "def process_image(file_path):\n",
    "    # Check if the file is an image (you can add more image extensions if necessary)\n",
    "    if file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "        # Read the image using OpenCV\n",
    "        image = cv2.imread(str(file_path))\n",
    "        \n",
    "        # Calculate the size of the smaller edge\n",
    "        height, width, _ = image.shape\n",
    "        smaller_edge = min(height, width)\n",
    "        \n",
    "        return smaller_edge < 64, file_path\n",
    "    else:\n",
    "        return False, False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Specify the path to the folder containing the images\n",
    "    folder_path = Path('/raid/utku/datasets/COCO_dataset/train2017/')\n",
    "    \n",
    "    # Initialize counters for images with edges smaller than 64 and 70\n",
    "    count_smaller_than_64 = 0\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    pool = multiprocessing.Pool(80)\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    results = pool.map(process_image, folder_path.glob('*'))\n",
    "\n",
    "    # Process the results\n",
    "    smaller_files =[]\n",
    "    for smaller_than_64, file_path in results:\n",
    "        if smaller_than_64:\n",
    "            count_smaller_than_64 += 1\n",
    "            smaller_files.append(file_path)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Number of images with edge smaller than 64: {count_smaller_than_64}\")\n",
    "    print(f\"smaller_files are {smaller_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.tensor([1]).to(7).device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK TEST CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state_dict from the .pt file\n",
    "from torchvision import models\n",
    "from uni_vip import UVIP\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "from typing import OrderedDict\n",
    "\n",
    "rank = 0\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12356' # set port for communication\n",
    "dist.init_process_group(backend='nccl',rank=0, world_size=1) # For multi GPU train.\n",
    "\n",
    "# If you're in a distributed environment, make sure all processes are synchronized before loading\n",
    "state_dict = torch.load(\"/home/kuartis-dgx1/utku/UniVIP/train/uni_vip_pretrained_model.pt\", map_location=lambda storage, loc: storage.cuda(0))\n",
    "# state_dict = model.state_dict()\n",
    "new_state_dict = {}\n",
    "\n",
    "module_key_prefix = \"module.online_encoder.net.\"\n",
    "for name, param in state_dict.items():\n",
    "    if name.startswith(module_key_prefix):\n",
    "        new_state_dict[name.replace(module_key_prefix, \"\")] = param\n",
    "\n",
    "new_linear_layer = torch.nn.Linear(2048, 1000)\n",
    "new_state_dict[\"fc.weight\"] = new_linear_layer.weight\n",
    "new_state_dict[\"fc.bias\"] = new_linear_layer.bias\n",
    "\n",
    "new_state_dict = OrderedDict(new_state_dict)\n",
    "resnet = models.resnet50(pretrained=False)\n",
    "# Assign the updated state_dict to the mode\n",
    "print(state_dict.keys())\n",
    "print(new_state_dict.keys())\n",
    "print(resnet.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.load_state_dict(new_state_dict)\n",
    "\n",
    "# Freeze previous layers\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Enable training for the new linear layer\n",
    "for param in resnet.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# model is ready\n",
    "# TODO \n",
    "# NOTE To let a non-DDP model load a state dict from a DDP model, consume_prefix_in_state_dict_if_present() needs to be applied to strip the prefix “module.” in the DDP state dict before loading.\n",
    "resnet = resnet.to(rank)\n",
    "resnet = DDP(resnet, device_ids=[rank])\n",
    "resnet = torch.nn.SyncBatchNorm.convert_sync_batchnorm(resnet) # Not sure if UniVIP does this.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load whole univip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state dictionary into the model\n",
    "model.load_state_dict({k: state_dict[k] for k in state_dict if k in model.state_dict()})\n",
    "\n",
    "# Optional: If you're in a distributed environment, make sure all processes are synchronized after loading\n",
    "if torch.distributed.is_initialized():\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "for name, param in state_dict.items():\n",
    "    if name.startswith(\"module.online_encoder\"):\n",
    "        state_dict[name.replace(\"module.online_encoder.\", \"\")] = param"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load only resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict\n",
    "state_dict = model.state_dict()\n",
    "new_state_dict = {}\n",
    "for name, param in state_dict.items():\n",
    "    if name.startswith(\"module.online_encoder\"):\n",
    "        new_state_dict[name.replace(\"module.online_encoder.net.\", \"\")] = param\n",
    "new_state_dict = OrderedDict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(pretrained=False)\n",
    "print(mod.state_dict().keys())\n",
    "print(new_state_dict.keys())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
