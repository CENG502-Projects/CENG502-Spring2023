{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS\n",
    "* BELOW I HAVE TESTED IF TRANSFORMS CALCULATE THE RANDOM PROBABILITY SEPERATELY FOR EVERY IMAGE IN A\n",
    "* ANSWER -> IT DOES NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = T.RandomResizedCrop((image_size, image_size))\n",
    "import torchvision\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "params= t.get_params(img, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333)) # returns top: int, left: int, height: int, width\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "t_color = T.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
    "params = t_color.get_params((0.2, 1.8), (0.2, 1.8), (0.2, 1.8), (-0.2, 0.2))\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "class RandomHorizontalFlip(torch.nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        prob = torch.rand(1)\n",
    "        print(prob)\n",
    "        if prob < self.p:\n",
    "            return F.hflip(img)\n",
    "        return img\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/dataset/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.shape)\n",
    "a = RandomHorizontalFlip()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "print(img.shape)\n",
    "_, height, width = F.get_dimensions(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "def random_hflip(img, p):\n",
    "    flip_bool = torch.rand(1) < p\n",
    "    return (F.hflip(img),flip_bool) if flip_bool else (img, flip_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "conv = torch.nn.Conv2d(3, 4, 2)\n",
    "\n",
    "at = torch.ones((3,4,3,60,60))\n",
    "print(at.size())\n",
    "bt = at.reshape(-1,*at.size()[-3:])\n",
    "print(bt.size())\n",
    "conv(bt).shape\n",
    "instance = F.interpolate(bt, size=(96, 96), mode=\"bicubic\") # resize instance to 96x96\n",
    "instance.shape\n",
    "torch.stack([bt,bt]).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming O and T are your input matrices of size KxM and KxN respectively.\n",
    "# Horizontal stack of column vectors of lenght K (K is number of instances)\n",
    "O_matrix = torch.tensor([[3,5,6],[4,12,8]], dtype=torch.float)# (M is number of features) \n",
    "T_matrix = torch.tensor([[1,2,3],[4,2,5]], dtype=torch.float)# (N is number of features)\n",
    "\n",
    "# Step 1: Compute DotProductMatrix\n",
    "DotProductMatrix = torch.mm(O_matrix.t(), T_matrix)\n",
    "\n",
    "# Step 2: Compute Norm matrices\n",
    "NormMatrixO = torch.norm(O_matrix, dim=0, keepdim=True)\n",
    "NormMatrixT = torch.norm(T_matrix, dim=0, keepdim=True)\n",
    "\n",
    "# Step 3: Compute C\n",
    "C = 1 - (DotProductMatrix / (NormMatrixO.t() * NormMatrixT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming O and T are your input matrices of size KxM and KxN respectively.\n",
    "# Horizontal stack of column vectors of lenght K (K is number of instances)\n",
    "O = torch.tensor([[3,5,6],[4,12,8]], dtype=torch.float)# (M is number of features) \n",
    "T = torch.tensor([[1,2,3],[4,2,5]], dtype=torch.float)# (N is number of features)\n",
    "\n",
    "# Step 1: Compute DotProductMatrix\n",
    "DotProductMatrix = torch.mm(O.t(), T)\n",
    "O.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute Norm matrices, output will be a horizontal vector.\n",
    "\n",
    "NormMatrixO = torch.norm(O, dim=0, keepdim=True) # find norm of columns\n",
    "NormMatrixT = torch.norm(T, dim=0, keepdim=True)\n",
    "NormMatrixO.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormMatrixT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO check if this (NormMatrixO.t() * NormMatrixT) matches the target indices.\n",
    "# Step 3: Compute C\n",
    "C = 1 - (DotProductMatrix / (NormMatrixO.t() * NormMatrixT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test concatentation   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.ops\n",
    "\n",
    "def get_concatenated_instances(img, overlapping_boxes):\n",
    "    # img shape should be (b, c, h, w) - batch, channels, height, width\n",
    "    # overlapping_boxes shape should be (b, n, 4) - batch, number of boxes, coordinates (x1, y1, x2, y2)\n",
    "\n",
    "    # Number of boxes per image\n",
    "    num_boxes = overlapping_boxes.size(1)\n",
    "    \n",
    "    # Create batch indices to be concatenated with boxes -> (batch_size*K), each box will have an index (showing where it belongs)\n",
    "    batch_indices = torch.arange(img.size(0), dtype=torch.float32).view(-1, 1).repeat(1, num_boxes).view(-1, 1)\n",
    "    print(batch_indices.shape)\n",
    "    print(batch_indices)\n",
    "    \n",
    "    # Reshape boxes for roi_align\n",
    "    boxes = overlapping_boxes.view(-1, 4) # Collect total number of boxes in the first dim (batch_size*K)\n",
    "    \n",
    "    print(boxes.shape)\n",
    "    # Concatenate batch indices with boxes, index shows which image in a batch each box belongs\n",
    "    boxes_with_indices = torch.cat([batch_indices, boxes], dim=1)\n",
    "    \n",
    "    # Crop and resize using roi_align\n",
    "    output_size = (96, 96)\n",
    "    instances = torchvision.ops.roi_align(img, boxes_with_indices, output_size)\n",
    "    \n",
    "    # Now instances tensor has shape (b * n, c, 96, 96), TODO you can reshape it if needed\n",
    "    return instances\n",
    "\n",
    "# Example usage\n",
    "b, c, h, w = 3, 3, 256, 256\n",
    "img_batch = torch.randn(b, c, h, w)\n",
    "overlapping_boxes_batch = torch.tensor([[[50, 50, 100, 100], [30, 30, 80, 80]], [[10, 10, 60, 60], [20, 20, 70, 70]], [[40, 40, 110, 110], [50, 50, 120, 120]]])\n",
    "\n",
    "# instances_batch = get_concatenated_instances(img_batch, overlapping_boxes_batch)\n",
    "# print(instances_batch.shape)  # Should be (b * n, c, 96, 96)\n",
    "img = torch.randn(1, c, h, w)\n",
    "overlapping_boxes = torch.tensor([[[50, 50, 100, 100], [30, 30, 80, 80]]])\n",
    "instances = get_concatenated_instances(img, overlapping_boxes)\n",
    "print(instances.shape)  # Should be (b * n, c, 96, 96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([*overlapping_boxes.shape[0:2], *instances.shape[-3:]])\n",
    "instances = instances.reshape(*overlapping_boxes.shape[0:2], *instances.shape[-3:])\n",
    "torch.concat(instances, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "T_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "dot_product_matrix = torch.matmul(O_matrix, T_matrix.transpose(1, 2))  # (batch_size, number of features, number of features)\n",
    "norm_vector_O = torch.norm(O_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "norm_vector_T = torch.norm(T_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "print(norm_vector_O.shape)\n",
    "print(norm_vector_T.shape)\n",
    "torch.matmul(norm_vector_O, norm_vector_T.transpose(1, 2)).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SINKHORN DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute dot_product_matrix\n",
    "online_pred_avg = torch.randn(3, 1, 5)  # (batch_size, instance numbers K, number of features)\n",
    "O_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "target_proj_avg = torch.randn(3, 1, 5)  # (batch_size, instance numbers K, number of features)\n",
    "T_matrix = torch.randn(3, 4, 5)  # (batch_size, instance numbers K, number of features)\n",
    "dot_product_matrix = torch.matmul(O_matrix, T_matrix.transpose(1, 2))  # (batch_size, instance numbers K, instance numbers K)\n",
    "# Step 2: Compute Norm matrices\n",
    "norm_vector_O = torch.norm(O_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "norm_vector_T = torch.norm(T_matrix, dim=-1, keepdim=True) # normalize over features\n",
    "# Step 3: Compute C\n",
    "ot_cosine_similarity_matrix = (dot_product_matrix / torch.matmul(norm_vector_O, norm_vector_T.transpose(1, 2)))\n",
    "cost_matrix = 1 - ot_cosine_similarity_matrix\n",
    "a_vector = torch.nn.functional.relu(torch.matmul(T_matrix, online_pred_avg.transpose(1, 2))) # (batch_size, instance numbers K, 1)\n",
    "print(a_vector.values.shape)\n",
    "print(torch.matmul(O_matrix, target_proj_avg.transpose(1, 2)).shape)\n",
    "b_vector = torch.nn.functional.relu(torch.matmul(O_matrix, target_proj_avg.transpose(1, 2))) # (batch_size, instance numbers K, 1)\n",
    "print(b_vector.shape)\n",
    "print(b_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = torch.randn(1, 4, 1)  # (batch_size, instance numbers K, number of features)\n",
    "at.squeeze(dim=-1).shape # demander a (batch_size, instance numbers K, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, instance numbers K, instance numbers K)\n",
    "optimal_plan_matrix = torch.randn(5, 4, 4)  # (batch_size, instance numbers K, number of features)\n",
    "ot_cosine_similarity_matrix = torch.randn(5, 4, 4)  # (batch_size, instance numbers K, number of features)\n",
    "torch.sum(-torch.mul(optimal_plan_matrix,ot_cosine_similarity_matrix), dim=(-2,-1)).mean() # Forces similar instance representations to be close to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "image_size = 224\n",
    "torch.randn(2, 3, image_size, image_size, device=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "import random\n",
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/data_ops/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "print(img.dtype)\n",
    "img[0,0,0] = 255\n",
    "torchvision.transforms.functional.adjust_brightness(img, brightness_factor=0.2)\n",
    "torchvision.transforms.functional.adjust_contrast(img, 0.2)\n",
    "torchvision.transforms.functional.adjust_saturation(img, 0.2)\n",
    "\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        return x if random.random() > self.p else self.fn(x)\n",
    "\n",
    "from torchvision import transforms as T\n",
    "import torch\n",
    "img = torch.rand(size=(1, 3, 224, 224))*255\n",
    "# img = img.unsqueeze(dim=0).broadcast_to((3,*img.shape))\n",
    "print(img.dtype)\n",
    "print((img).max())\n",
    "col_jit = RandomApply(T.ColorJitter(0.4, 0.4, 0.2, 0.1), p = 0.8)\n",
    "print(col_jit(img).max())\n",
    "print(T.ColorJitter(0.4, 0.4, 0.2, 0.1)(img).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.transforms.functional.adjust_brightness(img/255, brightness_factor=0.2)\n",
    "(img/255).dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(size=(3, 1, 3, 255, 255)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "im = cv2.imread(\"/home/kuartis-dgx1/utku/UniVIP/data_ops/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.io.read_image(\"/home/kuartis-dgx1/utku/UniVIP/data_ops/211110_sereflikochisar__dfas__3_rgb--image_on_00006383.png\")\n",
    "torchvision.transforms.functional.adjust_brightness(img, brightness_factor=0.2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK NUMBER OF SMALL IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images with edge smaller than 64: 2\n",
      "smaller_files are [PosixPath('/raid/utku/datasets/COCO_dataset/train2017/000000187714.jpg'), PosixPath('/raid/utku/datasets/COCO_dataset/train2017/000000363747.jpg')]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import multiprocessing\n",
    "\n",
    "def process_image(file_path):\n",
    "    # Check if the file is an image (you can add more image extensions if necessary)\n",
    "    if file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "        # Read the image using OpenCV\n",
    "        image = cv2.imread(str(file_path))\n",
    "        \n",
    "        # Calculate the size of the smaller edge\n",
    "        height, width, _ = image.shape\n",
    "        smaller_edge = min(height, width)\n",
    "        \n",
    "        return smaller_edge < 64, file_path\n",
    "    else:\n",
    "        return False, False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Specify the path to the folder containing the images\n",
    "    folder_path = Path('/raid/utku/datasets/COCO_dataset/train2017/')\n",
    "    \n",
    "    # Initialize counters for images with edges smaller than 64 and 70\n",
    "    count_smaller_than_64 = 0\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    pool = multiprocessing.Pool(80)\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    results = pool.map(process_image, folder_path.glob('*'))\n",
    "\n",
    "    # Process the results\n",
    "    smaller_files =[]\n",
    "    for smaller_than_64, file_path in results:\n",
    "        if smaller_than_64:\n",
    "            count_smaller_than_64 += 1\n",
    "            smaller_files.append(file_path)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Number of images with edge smaller than 64: {count_smaller_than_64}\")\n",
    "    print(f\"smaller_files are {smaller_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor([1]).to(7).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
